{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-846f2cd66c47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sqlite3\n",
    "import re\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Amazon review Data Set using pandas\n",
    "con = sqlite3.connect(\"D:\\Artificial Intelligence\\Real World Problem 1\\database.sqlite\")\n",
    "df = pd.read_sql_query(\"Select * from Reviews where score != 3\",con)\n",
    "\n",
    "\n",
    "df[\"Score\"] = [1 if x >=4 else 0 for x in df[\"Score\"].values]\n",
    "#Sorted the data set according to timestamp inorder to do the time base slicing\n",
    "df  =  df.sort_values(\"Time\")\n",
    "\n",
    "#Remove The Dublicate\n",
    "df  = df.drop_duplicates([\"ProfileName\",\"Time\",\"UserId\",\"Text\"],keep='first')\n",
    "df  = df[df[\"HelpfulnessNumerator\"]<=df[\"HelpfulnessDenominator\"]]\n",
    "# Text Preprocessing:\n",
    "\n",
    "words = set(stopwords.words('english'))\n",
    "snb = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTML(sentence):\n",
    "    compiled = re.compile(\"<.*?>\")\n",
    "    newSentence = re.sub(compiled, '', sentence)\n",
    "    return newSentence\n",
    "\n",
    "\n",
    "def removePunct(sentence):\n",
    "    compiled = re.compile(\"[!|[|.|,|/|)|(|^|`]\")\n",
    "    newSentence = re.sub(compiled, '', sentence)\n",
    "    return newSentence\n",
    "\n",
    "\n",
    "completeText = []\n",
    "arrangeText = ''\n",
    "df = df.iloc[:1000,:]\n",
    "\n",
    "for sentence in df[\"Text\"].values:\n",
    "    word_ar = []\n",
    "    sent = removeHTML(sentence)\n",
    "    for word in removePunct(sent).split():\n",
    "        if (len(word) >= 2):\n",
    "            if (word.lower() not in words):\n",
    "                word_ar.append(snb.stem(word.lower()))\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    arrangeText = ' '.join(word_ar)\n",
    "    completeText.append(arrangeText)\n",
    "\n",
    "df[\"newText\"] = completeText\n",
    "\n",
    "X = np.array(df[\"newText\"])\n",
    "Y = np.array(df[\"Score\"])\n",
    "\n",
    "# now train_test_split data\n",
    "x_tr,x_test,y_tr,y_test = train_test_split(X,Y,test_size=0.3,random_state=42)\n",
    "\n",
    "\n",
    "#MSE error Misclassification error\n",
    "\n",
    "def mseError(lis):\n",
    "    return [1-x for x in lis]\n",
    "\n",
    "# #Cross Validation function\n",
    "def crossValidation(neighbourse,xTrain,yTrain):\n",
    "    data  = []\n",
    "    for i in neighbourse:\n",
    "        knn = KNeighborsClassifier(n_neighbors=i, algorithm=\"kd_tree\")\n",
    "        scores = cross_val_score(knn, xTrain, yTrain, cv=10, scoring='accuracy')\n",
    "        data.append(scores.mean())\n",
    "    return data\n",
    "#\n",
    "# #find the optimal K\n",
    "def OptimalKFinder(nei,mse):\n",
    "    return nei[mse.index(min(mse))]\n",
    "\n",
    "#calculating the test accuracy\n",
    "def testAccuracy(xTrain,yTrain,optimalk,xTest,yTest):\n",
    "    neigh = KNeighborsClassifier(optimalk)\n",
    "    neigh.fit(xTrain, yTrain)\n",
    "    y_pred = neigh.predict(xTest)\n",
    "    print(accuracy_score(yTest, y_pred, normalize=True) * float(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe optimal number of neighbors is 7.\n89.6666666667\n"
     ]
    }
   ],
   "source": [
    "#BAG OF WORDS\n",
    "\n",
    "cv = CountVectorizer()\n",
    "denseMat  = cv.fit_transform(x_tr)\n",
    "\n",
    "svd =  TruncatedSVD(n_components=2)\n",
    "x_tr_mat = svd.fit_transform(denseMat)\n",
    "\n",
    "\n",
    "denseMattest = cv.fit_transform(x_test)\n",
    "svd =  TruncatedSVD(n_components=2)\n",
    "x_test_mat = svd.fit_transform(denseMattest)\n",
    "\n",
    "neighbourse = np.arange(1,50,2)\n",
    "bestK = crossValidation(neighbourse,x_tr_mat,y_tr)\n",
    "MSEe = mseError(bestK)\n",
    "\n",
    "# determining best k\n",
    "optimal_k = OptimalKFinder(neighbourse,MSEe)\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimal_k)\n",
    "\n",
    "testAccuracy(x_tr_mat,y_tr,optimal_k,x_test_mat,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe optimal number of neighbors is 7.\n89.6666666667\n"
     ]
    }
   ],
   "source": [
    "# Applying TFIDF\n",
    "\n",
    "tdfid =  TfidfVectorizer()\n",
    "denseTFIDmat = tdfid.fit_transform(x_tr)\n",
    "svdTF = TruncatedSVD(n_components=2)\n",
    "x_tfid_tr = svdTF.fit_transform(denseTFIDmat)\n",
    "\n",
    "\n",
    "denseTFIDTestMat =tdfid.fit_transform(x_test)\n",
    "svdTFtest = TruncatedSVD(n_components=2)\n",
    "x_tfid_test = svdTFtest.fit_transform(denseTFIDTestMat)\n",
    "\n",
    "\n",
    "neigbourTFID = np.arange(1,50,2)\n",
    "scoringListTFID = crossValidation(neigbourTFID,x_tfid_tr,y_tr)\n",
    "MSE = mseError(scoringListTFID)\n",
    "\n",
    "optimal_k_tfid = OptimalKFinder(neigbourTFID,MSE)\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimal_k_tfid)\n",
    "testAccuracy(x_tfid_tr,y_tr,optimal_k_tfid,x_tfid_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6a44383eef27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mtokizedData_test_data\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtokenizeData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokizedData_tr_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mx_tr_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetw2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokizedData_tr_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gensim' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Word to vector using the gensim\n",
    "\n",
    "\n",
    "def tokenizeData(data):\n",
    "    list_of_sentence = []\n",
    "    for sent in data:\n",
    "        filtered_sentence = []\n",
    "        for word in sent.split():\n",
    "            filtered_sentence.append(word)\n",
    "\n",
    "        list_of_sentence.append(filtered_sentence)\n",
    "\n",
    "    return list_of_sentence\n",
    "\n",
    "def getw2v(tokenizedData):\n",
    "    x_tr_data = []\n",
    "    for sent in tokenizedData:\n",
    "        sent_vec = np.zeros(50)\n",
    "        for word in sent:\n",
    "            try:\n",
    "                sent_vec += w2v.wv[word]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        x_tr_data.append(sent_vec)\n",
    "    return x_tr_data\n",
    "\n",
    "\n",
    "tokizedData_tr_data = tokenizeData(x_tr)\n",
    "tokizedData_test_data =  tokenizeData(x_test)\n",
    "\n",
    "w2v = gensim.models.Word2Vec(tokizedData_tr_data,min_count=5,size=50,workers=4)\n",
    "\n",
    "x_tr_vector = getw2v(tokizedData_tr_data)\n",
    "x_test_vector  = getw2v(tokizedData_test_data)\n",
    "\n",
    "svd_w2v_tr = TruncatedSVD(n_components=2)\n",
    "x_tr_svd = svd_w2v_tr.fit_transform(x_tr_vector)\n",
    "\n",
    "svd_w2v_test = TruncatedSVD(n_components=2)\n",
    "x_test_svd = svd_w2v_test.fit_transform(x_test_vector)\n",
    "\n",
    "neig = np.arange(1,50,2)\n",
    "data = crossValidation(neig,x_tr_svd,y_tr)\n",
    "\n",
    "MSE_Err = mseError(data)\n",
    "#Optimal K\n",
    "optimalKVal = OptimalKFinder(neig,MSE_Err)\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimalKVal)\n",
    "testAccuracy(x_tr_svd,y_tr,optimalKVal,x_test_svd,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANURICH\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'neig' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-56dbfeb4de10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mavg_w2v_data\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcrossValidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_avgw2v_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mMSE_Err_avg_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_w2v_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0moptimalKVal_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimalKFinder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMSE_Err_avg_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'neig' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Average Word To vector :\n",
    "\n",
    "\n",
    "def avgW2V(data):\n",
    "    avgw2v =[]\n",
    "    for sent in data:\n",
    "        sent_lis = np.zeros(50)\n",
    "        cnt =0\n",
    "        for word in sent:\n",
    "            try:\n",
    "                sent_lis += w2v.wv[word]\n",
    "                cnt +=1\n",
    "            except:\n",
    "              pass\n",
    "\n",
    "        sent_lis /= cnt\n",
    "        avgw2v.append(sent_lis)\n",
    "    return avgw2v\n",
    "\n",
    "x_avgw2v_tr = avgW2V(tokizedData_tr_data)\n",
    "x_avgw2v_test = avgW2V(tokizedData_test_data)\n",
    "\n",
    "\n",
    "avg_w2v_data= crossValidation(neig,x_avgw2v_tr,y_tr)\n",
    "MSE_Err_avg_w2v = mseError(avg_w2v_data)\n",
    "optimalKVal_w2v = OptimalKFinder(neig,MSE_Err_avg_w2v)\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimalKVal_w2v)\n",
    "testAccuracy(x_avgw2v_tr,y_tr,optimalKVal_w2v,x_avgw2v_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Average TFID\n",
    "\n",
    "features  = tdfid.get_feature_names()\n",
    "\n",
    "def avgTFIDF(data):\n",
    "    row = 0\n",
    "    avg_tfid =[]\n",
    "    for sent in data:\n",
    "        total_TFIDF = 0\n",
    "        w2v_lis = np.zeros(50)\n",
    "        for word in sent:\n",
    "           try:\n",
    "\n",
    "               wv_data = w2v.wv[word]\n",
    "               tfid = denseTFIDmat[row,features.index(word)]\n",
    "               newW2v = (wv_data*tfid)\n",
    "               total_TFIDF +=1\n",
    "               w2v_lis += newW2v\n",
    "           except:\n",
    "               pass\n",
    "\n",
    "\n",
    "        w2v_lis /= total_TFIDF\n",
    "        avg_tfid.append(w2v_lis)\n",
    "\n",
    "    return avg_tfid\n",
    "\n",
    "x_avgtfid_tr = avgTFIDF(tokizedData_tr_data)\n",
    "x_avgtfid_test=avgTFIDF(tokizedData_test_data)\n",
    "\n",
    "\n",
    "avg_avgtfid_data= crossValidation(neig,x_avgtfid_tr,y_tr)\n",
    "MSE_Err_avg_tfid = mseError(avg_avgtfid_data)\n",
    "optimalKVal_avgTFID = OptimalKFinder(neig,MSE_Err_avg_tfid)\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimalKVal_avgTFID)\n",
    "testAccuracy(x_avgtfid_tr,y_tr,optimalKVal_avgTFID,x_avgtfid_test,y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
